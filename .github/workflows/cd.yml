name: CD – Infra Apply/Destroy + ECS Deploy

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "apply or destroy"
        required: true
        type: choice
        options: [ apply, destroy ]
        default: apply
      imageSha:
        description: "Optional image SHA tag (default: last commit)"
        required: false
        type: string

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 097635932419.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPOSITORY: myapp
  CLUSTER_NAME: docker-ecs-deployment-cluster
  SERVICE_NAME: docker-ecs-deployment-svc
  IMAGE_TAG: latest
  IMAGE_SHA_TAG: ${{ inputs.imageSha || github.sha }}
  TG_NAME: docker-ecs-deployment-tg
  TF_WORKING_DIR: infra
  TF_PLUGIN_CACHE_DIR: $HOME/.terraform.d/plugin-cache

permissions:
  id-token: write
  contents: read

concurrency:
  group: cd-${{ github.ref }}-${{ github.event.inputs.mode || 'apply' }}
  cancel-in-progress: false

jobs:
  terraform:
    name: Terraform ${{ github.event.inputs.mode || 'apply' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS (terraform role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::097635932419:role/github-actions-terraform-role
          aws-region: ${{ env.AWS_REGION }}

      - name: WhoAmI (terraform)
        run: aws sts get-caller-identity

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      - name: Ensure TF plugin cache dir exists
        run: mkdir -p "${{ env.TF_PLUGIN_CACHE_DIR }}"

      - name: Restore Terraform plugin cache
        id: tf_cache_restore
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.TF_PLUGIN_CACHE_DIR }}
          key: tf-plugins-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('**/*.tf', '**/.terraform.lock.hcl') }}
          restore-keys: |
            tf-plugins-${{ runner.os }}-${{ runner.arch }}-

      - name: Terraform Init
        id: tf_init
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: ${{ env.TF_PLUGIN_CACHE_DIR }}
          TF_IN_AUTOMATION: 1
        run: terraform init -reconfigure -upgrade -input=false -no-color

      - name: Terraform Destroy
        if: ${{ github.event.inputs.mode == 'destroy' }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: ${{ env.TF_PLUGIN_CACHE_DIR }}
          TF_IN_AUTOMATION: 1
        run: terraform destroy -auto-approve -no-color

      - name: Terraform Plan
        if: ${{ github.event.inputs.mode != 'destroy' }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: ${{ env.TF_PLUGIN_CACHE_DIR }}
          TF_IN_AUTOMATION: 1
        run: terraform plan -input=false -no-color -out=tfplan

      - name: Terraform Apply
        if: ${{ github.event.inputs.mode != 'destroy' }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: ${{ env.TF_PLUGIN_CACHE_DIR }}
          TF_IN_AUTOMATION: 1
        run: terraform apply -input=false -no-color -auto-approve tfplan

      - name: Save Terraform plugin cache
        if: ${{ steps.tf_init.outcome == 'success' }}
        uses: actions/cache/save@v4
        with:
          path: ${{ env.TF_PLUGIN_CACHE_DIR }}
          key: tf-plugins-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('**/*.tf', '**/.terraform.lock.hcl') }}

  build_and_deploy:
    name: Build → Push → Register TD → Update Service → Smart Wait
    needs: terraform
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.mode != 'destroy' }}
    steps:
      - uses: actions/checkout@v4

      # Role with: ecr:GetAuthToken + (push to repo) + ecs:Register/Describe/Update + elb:Describe*
      - name: Configure AWS (ecs role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::097635932419:role/github-actions-ecs-role
          aws-region: ${{ env.AWS_REGION }}

      - name: WhoAmI (ecs)
        run: aws sts get-caller-identity

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      # Build & push two tags: latest and SHA (для TD используем именно SHA)
      - name: Build and push Docker image
        run: |
          set -euo pipefail
          docker build -f app/Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG app
          docker tag  $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_SHA_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_SHA_TAG

      - name: Ensure jq is installed
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Get current TaskDef ARN from Service
        id: svc
        run: |
          aws ecs describe-services \
            --cluster "${CLUSTER_NAME}" \
            --services "${SERVICE_NAME}" \
            --region "${AWS_REGION}" \
            --query "services[0].taskDefinition" \
            --output text > td_arn.txt
          echo "td_arn=$(cat td_arn.txt)" >> $GITHUB_OUTPUT

      - name: Get full TaskDef JSON
        run: |
          aws ecs describe-task-definition \
            --task-definition "${{ steps.svc.outputs.td_arn }}" \
            --region "${AWS_REGION}" \
            --query "taskDefinition" > taskdef.json
          cat taskdef.json

      - name: Build new TaskDef JSON with new image (SHA tag)
        id: build_td
        env:
          IMAGE_SHA: ${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_SHA_TAG }}
        run: |
          CN=$(jq -r '
            ([.containerDefinitions[] | select(.name=="app") | .name] + [.containerDefinitions[0].name])[0]
          ' taskdef.json)
          echo "Container name: $CN"

          jq --arg IMG "$IMAGE_SHA" --arg CN "$CN" '
            del(.revision, .status, .taskDefinitionArn, .requiresAttributes, .compatibilities, .registeredBy, .registeredAt)
            | .containerDefinitions = (.containerDefinitions
                | map(if .name == $CN then .image = $IMG else . end))
          ' taskdef.json > register.json

          echo "=== TaskDef to register ==="
          cat register.json

      - name: Register new TaskDef
        id: register_td
        run: |
          NEW_TD_ARN=$(aws ecs register-task-definition \
            --region "${AWS_REGION}" \
            --cli-input-json file://register.json \
            --query "taskDefinition.taskDefinitionArn" \
            --output text)
          echo "new_td_arn=${NEW_TD_ARN}" >> $GITHUB_OUTPUT
          echo "Registered: ${NEW_TD_ARN}"

      - name: Update Service to new TaskDef
        run: |
          aws ecs update-service \
            --cluster "${CLUSTER_NAME}" \
            --service "${SERVICE_NAME}" \
            --task-definition "${{ steps.register_td.outputs.new_td_arn }}" \
            --region "${AWS_REGION}"

      # SMART WAIT: печатает события ECS и здоровье таргетов ALB
      - name: Wait for service stability (smart logs)
        shell: bash
        run: |
          set -euo pipefail
          END=$((SECONDS+900)) # 15m timeout

          TG_ARN=$(aws elbv2 describe-target-groups \
            --names "$TG_NAME" \
            --region "$AWS_REGION" \
            --query "TargetGroups[0].TargetGroupArn" \
            --output text 2>/dev/null || echo "None")

          while [ $SECONDS -lt $END ]; do
            echo "— $(date -u +'%H:%M:%S') ECS service status"
            aws ecs describe-services \
              --cluster "$CLUSTER_NAME" \
              --services "$SERVICE_NAME" \
              --region "$AWS_REGION" \
              --query 'services[0].{
                desired:desiredCount,
                running:runningCount,
                pending:pendingCount,
                deployments:length(deployments),
                events:events[0:4].[message]
              }' \
              --output table || true

            if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
              echo "— $(date -u +'%H:%M:%S') ALB target health"
              aws elbv2 describe-target-health \
                --target-group-arn "$TG_ARN" \
                --region "$AWS_REGION" \
                --query 'TargetHealthDescriptions[].{ip:Target.Id,az:Target.AvailabilityZone,state:TargetHealth.State,reason:TargetHealth.Reason}' \
                --output table || true
            fi

            NOT_DONE=$(aws ecs describe-services \
              --cluster "$CLUSTER_NAME" \
              --services "$SERVICE_NAME" \
              --region "$AWS_REGION" \
              --query 'length(services[0].deployments[?rolloutState!=`COMPLETED`])' \
              --output text 2>/dev/null || echo 1)

            if [ "$NOT_DONE" = "0" ]; then
              echo "Service is stable ✅"
              exit 0
            fi
            sleep 20
          done

          echo "Timeout waiting for service stability ❌"
          exit 1