name: Infra + App (Apply / Destroy)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      mode:
        description: "Choose action: apply or destroy"
        required: true
        type: choice
        options: [ apply, destroy ]
        default: apply

# Global env for both jobs
env:
  AWS_REGION: us-east-1

  # Project naming (match your Terraform)
  PROJECT_NAME: docker-ecs-deployment
  CLUSTER_NAME: docker-ecs-deployment-cluster
  SERVICE_NAME: docker-ecs-deployment-svc
  ALB_NAME: docker-ecs-deployment-alb
  TG_NAME: docker-ecs-deployment-tg

  # ECR / image tags
  ECR_REGISTRY: 097635932419.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPOSITORY: myapp
  IMAGE_TAG: latest
  IMAGE_SHA_TAG: ${{ github.sha }}

  # Terraform paths
  TF_WORKING_DIR: infra
  TF_PLUGIN_CACHE_DIR: $HOME/.terraform.d/plugin-cache

permissions:
  id-token: write     # needed for OIDC
  contents: read

# Prevent overlapping runs for the same ref + action
concurrency:
  group: deploy-${{ github.ref }}-${{ github.event.inputs.mode || 'apply' }}
  cancel-in-progress: false

jobs:
  #####################################################################
  # 1) Terraform: apply (default) or destroy (via workflow_dispatch)
  #####################################################################
  terraform:
    name: Terraform (apply or destroy)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Broad role for Terraform (create/destroy AWS resources)
      - name: Configure AWS (terraform role via OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::097635932419:role/github-actions-terraform-role
          aws-region: ${{ env.AWS_REGION }}

      - name: WhoAmI (terraform)
        env: { AWS_PAGER: "" }
        run: aws sts get-caller-identity

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      # Ensure plugin cache dir exists before cache action
      - name: Ensure TF plugin cache dir exists
        run: mkdir -p "${TF_PLUGIN_CACHE_DIR}"

      # Restore TF plugin cache
      - name: Restore Terraform plugin cache
        id: tf_cache_restore
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.TF_PLUGIN_CACHE_DIR }}
          key: tf-plugins-${{ runner.os }}-${{ runner.arch }}-${{ github.ref_name }}-${{ hashFiles('**/*.tf', '**/*.tfvars', '**/.terraform.lock.hcl') }}
          restore-keys: |
            tf-plugins-${{ runner.os }}-${{ runner.arch }}-${{ github.ref_name }}-
            tf-plugins-${{ runner.os }}-${{ runner.arch }}-

      - name: Terraform Init
        id: tf_init
        working-directory: ${{ env.TF_WORKING_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: ${{ env.TF_PLUGIN_CACHE_DIR }}
          TF_IN_AUTOMATION: 1
        run: terraform init -reconfigure -upgrade -input=false -no-color

      # Optional hygiene before destroy to avoid "ResourceInUse"
      - name: Pre-destroy cleanup (drain ECS + detach ALB listeners)
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.mode == 'destroy' }}
        env:
          AWS_PAGER: ""
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        run: |
          set -e
          # Drain & remove ECS service if exists
          aws ecs update-service --cluster "$CLUSTER_NAME" --service "$SERVICE_NAME" --desired-count 0 --region "$AWS_DEFAULT_REGION" || true
          aws ecs delete-service  --cluster "$CLUSTER_NAME" --service "$SERVICE_NAME" --force         --region "$AWS_DEFAULT_REGION" || true

          # Make TG drain faster and deregister any residual targets
          TG_ARN=$(aws elbv2 describe-target-groups --names "$TG_NAME" \
            --query "TargetGroups[0].TargetGroupArn" --output text --region "$AWS_DEFAULT_REGION" 2>/dev/null || echo "None")
          if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
            aws elbv2 modify-target-group-attributes \
              --target-group-arn "$TG_ARN" \
              --attributes Key=deregistration_delay.timeout_seconds,Value=10 \
              --region "$AWS_DEFAULT_REGION" || true
            aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --region "$AWS_DEFAULT_REGION" \
              --query 'TargetHealthDescriptions[].Target' --output json > targets.json || true
            if [ -s targets.json ]; then
              aws elbv2 deregister-targets --target-group-arn "$TG_ARN" \
                --targets file://targets.json --region "$AWS_DEFAULT_REGION" || true
            fi
          fi

          # Delete ALB listeners so TG is not "in use"
          ALB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" \
            --query "LoadBalancers[0].LoadBalancerArn" --output text --region "$AWS_DEFAULT_REGION" 2>/dev/null || echo "None")
          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            for L in $(aws elbv2 describe-listeners \
              --load-balancer-arn "$ALB_ARN" --query "Listeners[].ListenerArn" \
              --output text --region "$AWS_DEFAULT_REGION" 2>/dev/null || true); do
              aws elbv2 delete-listener --listener-arn "$L" --region "$AWS_DEFAULT_REGION" || true
            done
          fi
          sleep 20

      - name: Terraform Destroy
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.mode == 'destroy' }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env: { TF_IN_AUTOMATION: 1 }
        run: terraform destroy -auto-approve -no-color

      - name: Terraform Plan
        if: ${{ !(github.event_name == 'workflow_dispatch' && inputs.mode == 'destroy') }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env: { TF_IN_AUTOMATION: 1 }
        run: terraform plan -input=false -no-color -out=tfplan

      - name: Terraform Apply
        if: ${{ !(github.event_name == 'workflow_dispatch' && inputs.mode == 'destroy') }}
        working-directory: ${{ env.TF_WORKING_DIR }}
        env: { TF_IN_AUTOMATION: 1 }
        run: terraform apply -input=false -no-color -auto-approve tfplan

      # Save cache only when init succeeded and there was no cache-hit (avoid "reserve" races)
      - name: Save Terraform plugin cache
        if: ${{ steps.tf_init.outcome == 'success' && steps.tf_cache_restore.outputs.cache-hit != 'true' }}
        uses: actions/cache/save@v4
        with:
          path: ${{ env.TF_PLUGIN_CACHE_DIR }}
          key: tf-plugins-${{ runner.os }}-${{ runner.arch }}-${{ github.ref_name }}-${{ hashFiles('**/*.tf', '**/*.tfvars', '**/.terraform.lock.hcl') }}

  #####################################################################
  # 2) Image build/push (plain docker) + ECS TaskDef update & rollout
  #####################################################################
  image_and_deploy:
    name: Build & Push Image, Update ECS TaskDef & Service
    needs: terraform
    runs-on: ubuntu-latest
    if: ${{ !(github.event_name == 'workflow_dispatch' && inputs.mode == 'destroy') }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Least-priv role for ECR push + ECS update
      - name: Configure AWS (ecs role via OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::097635932419:role/github-actions-ecs-role
          aws-region: ${{ env.AWS_REGION }}

      - name: WhoAmI (ecs)
        env: { AWS_PAGER: "" }
        run: aws sts get-caller-identity

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      # 🚫 No buildx. Plain docker build/push with AWS Public ECR base in your Dockerfile.
      - name: Build and push Docker image (plain docker)
        env:
          IMAGE_LATEST: "${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"
          IMAGE_SHA:    "${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_SHA_TAG }}"
        run: |
          set -euo pipefail
          echo "Building image: $IMAGE_LATEST"
          docker build -f app/Dockerfile -t "$IMAGE_LATEST" app
          docker tag "$IMAGE_LATEST" "$IMAGE_SHA"
          docker push "$IMAGE_LATEST"
          docker push "$IMAGE_SHA"
          echo "Image pushed:"
          echo "  latest => $IMAGE_LATEST"
          echo "  sha    => $IMAGE_SHA"

      - name: Ensure jq is installed
        run: sudo apt-get update && sudo apt-get install -y jq

      # Read current Task Definition ARN from the service
      - name: Describe current service (get task def)
        id: svc
        env: { AWS_PAGER: "" }
        run: |
          aws ecs describe-services \
            --cluster "${CLUSTER_NAME}" \
            --services "${SERVICE_NAME}" \
            --region "${AWS_REGION}" \
            --query "services[0].taskDefinition" \
            --output text > td_arn.txt
          echo "td_arn=$(cat td_arn.txt)" >> "$GITHUB_OUTPUT"

      - name: Get current task definition JSON (full)
        env: { AWS_PAGER: "" }
        run: |
          aws ecs describe-task-definition \
            --task-definition "${{ steps.svc.outputs.td_arn }}" \
            --region "${AWS_REGION}" \
            --query "taskDefinition" > taskdef.json
          cat taskdef.json

      # Replace only the image of a chosen container ("app" if exists, else first)
      - name: Build new task definition JSON with updated image
        id: build_td
        env:
          IMAGE_SHA: "${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_SHA_TAG }}"
        run: |
          CN=$(jq -r '
            ([.containerDefinitions[] | select(.name=="app") | .name] + [.containerDefinitions[0].name])[0]
          ' taskdef.json)
          echo "Container name: $CN"

          jq --arg IMG "$IMAGE_SHA" --arg CN "$CN" '
            del(.revision, .status, .taskDefinitionArn, .requiresAttributes, .compatibilities, .registeredBy, .registeredAt)
            | .containerDefinitions = (.containerDefinitions
                | map(if .name == $CN then .image = $IMG else . end))
          ' taskdef.json > register.json

          echo "=== TaskDef to register ==="
          cat register.json

      # Fast, explicit retries around aws register-task-definition
      - name: Register new task definition (fast-retry)
        id: register_td
        timeout-minutes: 2
        env:
          AWS_PAGER: ""
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
          AWS_MAX_ATTEMPTS: "2"   # reduce internal CLI retries
          AWS_RETRY_MODE: adaptive
        run: |
          set -euo pipefail
          tries=8
          sleep_s=2
          for i in $(seq 1 $tries); do
            echo "Attempt $i/$tries: aws ecs register-task-definition ..."
            if NEW_TD_ARN=$(aws ecs register-task-definition \
                  --cli-input-json file://register.json \
                  --query "taskDefinition.taskDefinitionArn" \
                  --output text 2>err.txt); then
              echo "new_td_arn=${NEW_TD_ARN}" >> $GITHUB_OUTPUT
              echo "Registered: ${NEW_TD_ARN}"
              exit 0
            fi
            code=$?
            echo "Register failed (exit=$code):"
            cat err.txt || true
            if [ $i -lt $tries ]; then
              echo "Retry after ${sleep_s}s..."
              sleep $sleep_s
              sleep_s=$(( sleep_s*2 ))
              if [ $sleep_s -gt 20 ]; then sleep_s=20; fi
            else
              echo "Giving up after $tries attempts."
              exit $code
            fi
          done

      - name: Update service to new task definition
        env: { AWS_PAGER: "" }
        run: |
          aws ecs update-service \
            --cluster "$CLUSTER_NAME" \
            --service "$SERVICE_NAME" \
            --task-definition "${{ steps.register_td.outputs.new_td_arn }}" \
            --region "$AWS_REGION"

      # Verbose progress instead of a silent "wait services-stable"
      - name: Wait for ECS service to become stable (with progress)
        timeout-minutes: 12
        env:
          AWS_PAGER: ""
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        run: |
          set -euo pipefail
          CL="$CLUSTER_NAME"; SV="$SERVICE_NAME"; RG="$AWS_DEFAULT_REGION"

          TG_ARN=$(aws ecs describe-services \
            --cluster "$CL" --services "$SV" --region "$RG" \
            --query "services[0].loadBalancers[0].targetGroupArn" --output text 2>/dev/null || true)
          if [ -z "$TG_ARN" ] || [ "$TG_ARN" = "None" ]; then
            TG_ARN=$(aws elbv2 describe-target-groups --names "$TG_NAME" \
              --query "TargetGroups[0].TargetGroupArn" --output text --region "$RG" 2>/dev/null || true)
          fi
          echo "TG_ARN=${TG_ARN:-<none>}"

          for i in $(seq 1 30); do
            SRV_JSON=$(aws ecs describe-services --cluster "$CL" --services "$SV" --region "$RG" --query "services[0]")

            DESIRED=$(echo "$SRV_JSON" | jq -r '.desiredCount // 0')
            RUNNING=$(echo "$SRV_JSON" | jq -r '.runningCount // 0')
            PENDING=$(echo "$SRV_JSON" | jq -r '.pendingCount // 0')
            DEPLOY=$(echo "$SRV_JSON" | jq -r '.deployments | length')
            PRIMARY_READY=$(echo "$SRV_JSON" | jq -r '[.deployments[] | select(.status=="PRIMARY")][0] | ( .runningCount==.desiredCount )')

            echo "— [$(date +%H:%M:%S)] desired=${DESIRED}, running=${RUNNING}, pending=${PENDING}, deployments=${DEPLOY}"

            if [ -n "${TG_ARN:-}" ] && [ "$TG_ARN" != "None" ]; then
              TH_JSON=$(aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --region "$RG" || true)
              H=$(echo "$TH_JSON" | jq '[.TargetHealthDescriptions[] | select(.TargetHealth.State=="healthy")] | length')
              I=$(echo "$TH_JSON" | jq '[.TargetHealthDescriptions[] | select(.TargetHealth.State=="initial")] | length')
              U=$(echo "$TH_JSON" | jq '[.TargetHealthDescriptions[] | select(.TargetHealth.State=="unhealthy")] | length')
              D=$(echo "$TH_JSON" | jq '[.TargetHealthDescriptions[] | select(.TargetHealth.State=="draining")] | length')
              T=$((H+I+U+D))
              echo "   ALB target health: total=${T} healthy=${H} initial=${I} unhealthy=${U} draining=${D}"
            fi

            LAST_EVENT=$(echo "$SRV_JSON" | jq -r '.events[0]?.message // ""')
            if [ -n "$LAST_EVENT" ]; then
              echo "   event: $LAST_EVENT"
            fi

            if [ "$RUNNING" -eq "$DESIRED" ] && [ "$PENDING" -eq 0 ] && [ "$DEPLOY" -eq 1 ] && [ "$PRIMARY_READY" = "true" ]; then
              echo "✅ Service is stable."
              exit 0
            fi
            sleep 20
          done

          echo "❌ Timed out waiting for service to stabilize."
          echo "Diagnostics:"
          aws ecs describe-services --cluster "$CL" --services "$SV" --region "$RG"
          if [ -n "${TG_ARN:-}" ] && [ "$TG_ARN" != "None" ]; then
            aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --region "$RG" || true
          fi
          exit 1